name: Terminal-Bench

on:
  workflow_call:
    inputs:
      model_name:
        description: "Model to use (e.g., anthropic/claude-opus-4-5)"
        required: false
        type: string
      thinking_level:
        description: "Thinking level (off, low, medium, high)"
        required: false
        type: string
      dataset:
        description: "Terminal-Bench dataset to use"
        required: false
        type: string
        default: "terminal-bench@2.0"
      concurrency:
        description: "Number of concurrent tasks (--n-concurrent)"
        required: false
        type: string
        default: "4"
      env:
        description: "Harbor backend environment (e.g., docker, daytona, e2b). Leave empty for Harbor default."
        required: false
        type: string
        default: ""
      task_names:
        description: "Space-separated task names to run (empty = all tasks)"
        required: false
        type: string
      extra_args:
        description: "Additional arguments to pass to harbor"
        required: false
        type: string
      experiments:
        description: "Experiments to enable (comma-separated)"
        required: false
        type: string
      timeout:
        description: "Agent timeout in seconds (default: 1800 = 30 min)"
        required: false
        type: string
        default: ""
      max_tasks:
        description: "Maximum number of tasks to run (for faster iteration)"
        required: false
        type: string
        default: ""
      mux_project_path:
        description: "Project path inside the task container (e.g., /testbed, /app/src)"
        required: false
        type: string
        default: ""
      mux_runtime:
        description: "Mux runtime override passed to mux-run.sh (e.g., local)"
        required: false
        type: string
        default: ""
    secrets:
      ANTHROPIC_API_KEY:
        required: true
      OPENAI_API_KEY:
        required: true
      GOOGLE_API_KEY:
        required: false
      DAYTONA_API_KEY:
        required: false
      GCP_SA_KEY:
        required: false
  workflow_dispatch:
    inputs:
      dataset:
        description: "Terminal-Bench dataset to use"
        required: false
        default: "terminal-bench@2.0"
        type: string
      concurrency:
        description: "Number of concurrent tasks (--n-concurrent)"
        required: false
        default: "4"
        type: string
      env:
        description: "Harbor backend environment (e.g., docker, daytona, e2b). Leave empty for Harbor default."
        required: false
        default: ""
        type: string
      task_names:
        description: "Space-separated task names to run (empty = all tasks)"
        required: false
        type: string
      model_name:
        description: "Model to use (e.g., anthropic/claude-opus-4-5, openai/gpt-5.2)"
        required: false
        type: string
      thinking_level:
        description: "Thinking level (off, low, medium, high)"
        required: false
        type: string
      extra_args:
        description: "Additional arguments to pass to harbor"
        required: false
        type: string
      experiments:
        description: "Experiments to enable (comma-separated)"
        required: false
        type: string
      timeout:
        description: "Agent timeout in seconds (default: 1800 = 30 min)"
        required: false
        type: string
      max_tasks:
        description: "Maximum number of tasks to run (for faster iteration)"
        required: false
        type: string

jobs:
  benchmark:
    name: Run Terminal-Bench${{ inputs.model_name && format(' ({0})', inputs.model_name) || '' }}
    runs-on: ${{ github.repository_owner == 'coder' && 'depot-ubuntu-22.04-16' || 'ubuntu-latest' }}
    # Full suite (~80 tasks) at concurrency=4 takes ~60-90 minutes typically
    # Set 4-hour timeout to handle occasional API slowdowns while preventing infinite hangs
    # If consistently hitting this timeout, investigate task-level issues
    timeout-minutes: 240
    steps:
      - name: Validate Harbor env input
        if: ${{ inputs.env == 'local' }}
        run: |
          echo "inputs.env=local is no longer supported by Harbor."
          echo "Use env: docker or env: daytona (or leave empty for Harbor default)."
          exit 1

      - name: Checkout code
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4.3.1
        with:
          fetch-depth: 0 # Required for git describe to find tags
          persist-credentials: false

      - uses: ./.github/actions/setup-mux

      - name: Install uv
        run: curl -LsSf https://astral.sh/uv/install.sh | sh

      - name: Add uv to PATH
        run: echo "$HOME/.local/bin" >> "$GITHUB_PATH"

      - name: Generate version file
        run: ./scripts/generate-version.sh

      - name: Build dist/ (skip icons - not needed for benchmark)
        run: make build-main build-preload

      - name: Run Terminal-Bench
        run: make benchmark-terminal 2>&1 | tee benchmark.log
        env:
          TB_DATASET: ${{ inputs.dataset }}
          TB_CONCURRENCY: ${{ inputs.concurrency }}
          TB_ENV: ${{ inputs.env }}
          TB_TASK_NAMES: ${{ inputs.task_names }}
          TB_MODEL: ${{ inputs.model_name }}
          MUX_MODEL: ${{ inputs.model_name }}
          TB_TIMEOUT: ${{ inputs.timeout }}
          MUX_PROJECT_PATH: ${{ inputs.mux_project_path }}
          MUX_RUNTIME: ${{ inputs.mux_runtime }}
          TB_ARGS: >-
            ${{ inputs.thinking_level && format('--agent-kwarg thinking_level={0}', inputs.thinking_level) || '' }}
            ${{ inputs.max_tasks && format('--n-tasks {0}', inputs.max_tasks) || '' }}
            ${{ inputs.extra_args || '' }}
          MUX_EXPERIMENTS: ${{ inputs.experiments }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          GOOGLE_GENERATIVE_AI_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          DAYTONA_API_KEY: ${{ secrets.DAYTONA_API_KEY }}

      - name: Print results summary
        if: always()
        run: |
          echo "=== Terminal-Bench Results Summary ==="
          # Harbor writes to jobs/<timestamp>/result.json (singular)
          if [ -f "$(find jobs -maxdepth 2 -name 'result.json' 2>/dev/null | head -1)" ]; then
            RESULTS_FILE=$(find jobs -maxdepth 2 -name 'result.json' | head -1)
            jq '.' "$RESULTS_FILE" 2>/dev/null || cat "$RESULTS_FILE"
          else
            echo "❌ No result.json found"
            ls -laR jobs/ 2>/dev/null || echo "jobs/ directory missing"
          fi

      - name: Verify agent ran successfully
        run: |
          # Harbor writes to jobs/<timestamp>/result.json
          RESULTS_FILE=$(find jobs -maxdepth 2 -name 'result.json' 2>/dev/null | head -1)
          if [ -z "$RESULTS_FILE" ]; then
            echo "❌ No result.json found - benchmark failed to produce results"
            exit 1
          fi
          # Harbor format: n_total_trials, stats.n_trials, stats.n_errors, stats.evals.<key>.metrics[0].mean
          N_TRIALS=$(jq -r '.n_total_trials // .stats.n_trials // 0' "$RESULTS_FILE")
          N_ERRORS=$(jq -r '.stats.n_errors // 0' "$RESULTS_FILE")
          # Get mean score from first eval entry (pass rate 0-1)
          MEAN_SCORE=$(jq -r '[.stats.evals | to_entries[].value.metrics[0].mean] | add // 0' "$RESULTS_FILE")
          echo "Trials: $N_TRIALS, Errors: $N_ERRORS, Mean score: $MEAN_SCORE"
          if [ "$N_TRIALS" -eq 0 ]; then
            echo "❌ No trials ran - benchmark configuration may be wrong"
            exit 1
          fi
          # Count infrastructure/timeout errors separately from other errors
          # These error types are expected and should not fail the workflow:
          # - AgentTimeoutError: Agent exceeded task time limit (hard tasks)
          # - VerifierTimeoutError: Verification timed out
          # - AgentSetupTimeoutError: Agent install.sh timed out (slow network/packages)
          # - EnvironmentStartTimeoutError: Daytona environment slow to start
          # - DaytonaError: Daytona infrastructure issues (sandbox not started, no IP, etc.)
          INFRA_PATTERN="(AgentTimeoutError|VerifierTimeoutError|AgentSetupTimeoutError|EnvironmentStartTimeoutError|DaytonaError)"
          INFRA_ERRORS=$(find jobs -name 'exception.txt' -exec grep -lE "$INFRA_PATTERN" {} \; 2>/dev/null | wc -l | tr -d ' ')
          OTHER_ERRORS=$((N_ERRORS - INFRA_ERRORS))
          # Clamp to 0 in case exception.txt files exceed n_errors (defensive)
          if [ "$OTHER_ERRORS" -lt 0 ]; then OTHER_ERRORS=0; fi
          if [ "$OTHER_ERRORS" -gt 0 ]; then
            echo "❌ $OTHER_ERRORS task(s) had non-infrastructure errors - agent execution failed"
            echo "--- Exception details (excluding infra errors) ---"
            find jobs -name 'exception.txt' -exec sh -c 'grep -qE "'"$INFRA_PATTERN"'" "$1" || { echo "=== $1 ===" && cat "$1"; }' _ {} \; 2>/dev/null || true
            exit 1
          fi
          if [ "$INFRA_ERRORS" -gt 0 ]; then
            echo "⏱️ $INFRA_ERRORS task(s) had infrastructure/timeout errors (expected for hard tasks or transient infra issues)"
          fi
          echo "✅ Agent ran $N_TRIALS trial(s) successfully (mean score: $MEAN_SCORE)"

      - name: Set artifact name
        if: always()
        id: artifact-name
        env:
          MODEL_NAME: ${{ inputs.model_name }}
          DATASET: ${{ inputs.dataset }}
          TASK_NAMES: ${{ inputs.task_names }}
          RUN_ID: ${{ github.run_id }}
          RUN_ATTEMPT: ${{ github.run_attempt }}
        run: |
          # actions/upload-artifact@v4 requires artifact names to be unique per workflow run.
          # Nightly workflows can run multiple jobs with the same model (e.g. Harbor smoke tests),
          # so include dataset + task selection in the artifact name.
          sanitize() { printf "%s" "$1" | tr ' :/@' '-' | tr -cd 'A-Za-z0-9._-'; }

          SAFE_MODEL=$(sanitize "${MODEL_NAME:-no-model}")
          SAFE_DATASET=$(sanitize "${DATASET:-unknown-dataset}")
          if [ -n "${TASK_NAMES:-}" ]; then
            SAFE_TASKS=$(sanitize "$TASK_NAMES")
          else
            SAFE_TASKS="all-tasks"
          fi

          ARTIFACT_NAME="terminal-bench-results-${SAFE_MODEL}-${SAFE_DATASET}-${SAFE_TASKS}-${RUN_ID}-a${RUN_ATTEMPT}"
          echo "name=$ARTIFACT_NAME" >> "$GITHUB_OUTPUT"
          echo "Artifact name: $ARTIFACT_NAME"

      - name: Upload Terminal-Bench results to BigQuery
        if: always() && github.repository == 'coder/mux' && startsWith(inputs.dataset, 'terminal-bench@')
        env:
          GCP_SA_KEY: ${{ secrets.GCP_SA_KEY }}
          GCP_PROJECT_ID: mux-benchmarks
          BQ_DATASET: benchmarks
          MUX_EXPERIMENTS: ${{ inputs.experiments }}
        run: |
          if [ -z "$GCP_SA_KEY" ]; then
            echo "GCP_SA_KEY not set, skipping BigQuery upload"
            exit 0
          fi
          echo "$GCP_SA_KEY" > /tmp/gcp-sa.json
          export GOOGLE_APPLICATION_CREDENTIALS=/tmp/gcp-sa.json
          pip install --quiet google-cloud-bigquery
          python scripts/upload-tbench-results.py
          rm -f /tmp/gcp-sa.json

      - name: Upload Harbor results to BigQuery
        if: always() && github.repository == 'coder/mux' && !startsWith(inputs.dataset, 'terminal-bench@')
        env:
          GCP_SA_KEY: ${{ secrets.GCP_SA_KEY }}
          GCP_PROJECT_ID: mux-benchmarks
          BQ_DATASET: benchmarks
          MUX_EXPERIMENTS: ${{ inputs.experiments }}
        run: |
          if [ -z "$GCP_SA_KEY" ]; then
            echo "GCP_SA_KEY not set, skipping BigQuery upload"
            exit 0
          fi
          echo "$GCP_SA_KEY" > /tmp/gcp-sa.json
          export GOOGLE_APPLICATION_CREDENTIALS=/tmp/gcp-sa.json
          pip install --quiet google-cloud-bigquery
          python scripts/upload-harbor-results.py
          rm -f /tmp/gcp-sa.json

      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        with:
          name: ${{ steps.artifact-name.outputs.name }}
          path: |
            jobs/
            benchmark.log
          if-no-files-found: warn
          retention-days: 30
